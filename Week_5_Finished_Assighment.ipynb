{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165166dd",
   "metadata": {},
   "source": [
    "# DS Automation Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195af74",
   "metadata": {},
   "source": [
    "Using our prepared churn data from week 2:\n",
    "- use pycaret to find an ML algorithm that performs best on the data\n",
    "    - Choose a metric you think is best to use for finding the best model; by default, it is accuracy but it could be AUC, precision, recall, etc. The week 3 FTE has some information on these different metrics.\n",
    "- save the model to disk\n",
    "- create a Python script/file/module with a function that takes a pandas dataframe as an input and returns the probability of churn for each row in the dataframe\n",
    "    - your Python file/function should print out the predictions for new data (new_churn_data.csv)\n",
    "    - the true values for the new data are [1, 0, 0, 1, 0] if you're interested\n",
    "- test your Python module and function with the new data, new_churn_data.csv\n",
    "- write a short summary of the process and results at the end of this notebook\n",
    "- upload this Jupyter Notebook and Python file to a Github repository, and turn in a link to the repository in the week 5 assignment dropbox\n",
    "\n",
    "*Optional* challenges:\n",
    "- return the probability of churn for each new prediction, and the percentile where that prediction is in the distribution of probability predictions from the training dataset (e.g. a high probability of churn like 0.78 might be at the 90th percentile)\n",
    "- use other autoML packages, such as TPOT, H2O, MLBox, etc, and compare performance and features with pycaret\n",
    "- create a class in your Python module to hold the functions that you created\n",
    "- accept user input to specify a file using a tool such as Python's `input()` function, the `click` package for command-line arguments, or a GUI\n",
    "- Use the unmodified churn data (new_unmodified_churn_data.csv) in your Python script. This will require adding the same preprocessing steps from week 2 since this data is like the original unmodified dataset from week 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648fc8a",
   "metadata": {},
   "source": [
    "### MSDS600_X700 Week 5 Assignment\n",
    "----------------------------------------------\n",
    "#### Student: Robert Apple\n",
    "#### Date: 21-July-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8557ba5",
   "metadata": {},
   "source": [
    "### 1a.  Use pycaret to find an ML algorithm that performs best on the data  \n",
    "\n",
    "#### I'm going to create two dataframes.  1) df_original.  2) df_modified\n",
    "\n",
    "1.  This one will be the original churn data, with the addition of the \"charge_per_tenure\"\n",
    "2.  This one will be modified by hand to 'clean it up'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "476d4ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7043 entries, 7590-VHVEG to 3186-AJIEK\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   tenure             7043 non-null   int64  \n",
      " 1   PhoneService       7043 non-null   int64  \n",
      " 2   Contract           7043 non-null   int64  \n",
      " 3   PaymentMethod      7043 non-null   int64  \n",
      " 4   MonthlyCharges     7043 non-null   float64\n",
      " 5   TotalCharges       7043 non-null   float64\n",
      " 6   charge_per_tenure  7043 non-null   float64\n",
      " 7   Churn              7043 non-null   int64  \n",
      "dtypes: float64(3), int64(5)\n",
      "memory usage: 495.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#--Loading up the orignal data and making small changes where warranted.\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "df = pd.read_csv('C:\\pandas_data\\churn_updated_data.csv', index_col = 'customerID')\n",
    "df['charge_per_tenure'] = pd.Series(df['TotalCharges']/df['tenure'])\n",
    "df = df[['tenure','PhoneService','Contract','PaymentMethod','MonthlyCharges','TotalCharges','charge_per_tenure','Churn']]\n",
    "\n",
    "\n",
    "df['TotalCharges'].fillna(0, inplace=True)\n",
    "df['charge_per_tenure'].fillna(0, inplace=True)\n",
    "\n",
    "df.to_csv('C:/Users/Rob4H/MSDS600_X70 -- Week 5/week_5_data.csv')\n",
    "\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7b601dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5 entries, 9305-CKSKC to 6348-TACGU\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   tenure             5 non-null      int64  \n",
      " 1   PhoneService       5 non-null      int64  \n",
      " 2   Contract           5 non-null      int64  \n",
      " 3   PaymentMethod      5 non-null      int64  \n",
      " 4   MonthlyCharges     5 non-null      float64\n",
      " 5   TotalCharges       5 non-null      float64\n",
      " 6   charge_per_tenure  5 non-null      float64\n",
      "dtypes: float64(3), int64(4)\n",
      "memory usage: 320.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#--modify the new data as well\n",
    "#-----------------------------\n",
    "new_data = pd.read_csv('C:/Users/Rob4H/MSDS600_X70 -- Week 5/new_churn_data.csv', index_col='customerID')\n",
    "\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7835bb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_92272_ th {\n",
       "          text-align: left;\n",
       "    }#T_92272_row0_col0,#T_92272_row0_col1,#T_92272_row0_col3,#T_92272_row0_col4,#T_92272_row0_col5,#T_92272_row0_col6,#T_92272_row0_col7,#T_92272_row1_col0,#T_92272_row1_col2,#T_92272_row1_col3,#T_92272_row1_col4,#T_92272_row1_col5,#T_92272_row2_col0,#T_92272_row2_col1,#T_92272_row2_col2,#T_92272_row2_col3,#T_92272_row2_col4,#T_92272_row2_col5,#T_92272_row2_col6,#T_92272_row2_col7,#T_92272_row3_col0,#T_92272_row3_col1,#T_92272_row3_col2,#T_92272_row3_col3,#T_92272_row3_col4,#T_92272_row3_col5,#T_92272_row3_col6,#T_92272_row3_col7,#T_92272_row4_col0,#T_92272_row4_col1,#T_92272_row4_col2,#T_92272_row4_col3,#T_92272_row4_col4,#T_92272_row4_col5,#T_92272_row4_col6,#T_92272_row4_col7,#T_92272_row5_col0,#T_92272_row5_col1,#T_92272_row5_col2,#T_92272_row5_col3,#T_92272_row5_col4,#T_92272_row5_col5,#T_92272_row5_col6,#T_92272_row5_col7,#T_92272_row6_col0,#T_92272_row6_col1,#T_92272_row6_col2,#T_92272_row6_col4,#T_92272_row6_col6,#T_92272_row6_col7,#T_92272_row7_col0,#T_92272_row7_col1,#T_92272_row7_col2,#T_92272_row7_col3,#T_92272_row7_col4,#T_92272_row7_col5,#T_92272_row7_col6,#T_92272_row7_col7,#T_92272_row8_col0,#T_92272_row8_col1,#T_92272_row8_col2,#T_92272_row8_col3,#T_92272_row8_col4,#T_92272_row8_col5,#T_92272_row8_col6,#T_92272_row8_col7,#T_92272_row9_col0,#T_92272_row9_col1,#T_92272_row9_col2,#T_92272_row9_col3,#T_92272_row9_col4,#T_92272_row9_col5,#T_92272_row9_col6,#T_92272_row9_col7,#T_92272_row10_col0,#T_92272_row10_col1,#T_92272_row10_col2,#T_92272_row10_col3,#T_92272_row10_col4,#T_92272_row10_col5,#T_92272_row10_col6,#T_92272_row10_col7,#T_92272_row11_col0,#T_92272_row11_col1,#T_92272_row11_col2,#T_92272_row11_col3,#T_92272_row11_col4,#T_92272_row11_col5,#T_92272_row11_col6,#T_92272_row11_col7,#T_92272_row12_col0,#T_92272_row12_col1,#T_92272_row12_col2,#T_92272_row12_col3,#T_92272_row12_col4,#T_92272_row12_col5,#T_92272_row12_col6,#T_92272_row12_col7,#T_92272_row13_col0,#T_92272_row13_col1,#T_92272_row13_col2,#T_92272_row13_col3,#T_92272_row13_col4,#T_92272_row13_col5,#T_92272_row13_col6,#T_92272_row13_col7,#T_92272_row14_col0,#T_92272_row14_col1,#T_92272_row14_col2,#T_92272_row14_col3,#T_92272_row14_col5,#T_92272_row14_col6,#T_92272_row14_col7{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "        }#T_92272_row0_col2,#T_92272_row1_col1,#T_92272_row1_col6,#T_92272_row1_col7,#T_92272_row6_col3,#T_92272_row6_col5,#T_92272_row14_col4{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "            background-color:  yellow;\n",
       "        }#T_92272_row0_col8,#T_92272_row1_col8,#T_92272_row2_col8,#T_92272_row3_col8,#T_92272_row4_col8,#T_92272_row5_col8,#T_92272_row6_col8,#T_92272_row7_col8,#T_92272_row8_col8,#T_92272_row9_col8,#T_92272_row10_col8,#T_92272_row11_col8,#T_92272_row12_col8,#T_92272_row13_col8{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "            background-color:  lightgrey;\n",
       "        }#T_92272_row14_col8{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "            background-color:  yellow;\n",
       "            background-color:  lightgrey;\n",
       "        }</style><table id=\"T_92272_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Model</th>        <th class=\"col_heading level0 col1\" >Accuracy</th>        <th class=\"col_heading level0 col2\" >AUC</th>        <th class=\"col_heading level0 col3\" >Recall</th>        <th class=\"col_heading level0 col4\" >Prec.</th>        <th class=\"col_heading level0 col5\" >F1</th>        <th class=\"col_heading level0 col6\" >Kappa</th>        <th class=\"col_heading level0 col7\" >MCC</th>        <th class=\"col_heading level0 col8\" >TT (Sec)</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_92272_level0_row0\" class=\"row_heading level0 row0\" >gbc</th>\n",
       "                        <td id=\"T_92272_row0_col0\" class=\"data row0 col0\" >Gradient Boosting Classifier</td>\n",
       "                        <td id=\"T_92272_row0_col1\" class=\"data row0 col1\" >0.7917</td>\n",
       "                        <td id=\"T_92272_row0_col2\" class=\"data row0 col2\" >0.8336</td>\n",
       "                        <td id=\"T_92272_row0_col3\" class=\"data row0 col3\" >0.4777</td>\n",
       "                        <td id=\"T_92272_row0_col4\" class=\"data row0 col4\" >0.6406</td>\n",
       "                        <td id=\"T_92272_row0_col5\" class=\"data row0 col5\" >0.5459</td>\n",
       "                        <td id=\"T_92272_row0_col6\" class=\"data row0 col6\" >0.4149</td>\n",
       "                        <td id=\"T_92272_row0_col7\" class=\"data row0 col7\" >0.4231</td>\n",
       "                        <td id=\"T_92272_row0_col8\" class=\"data row0 col8\" >0.1890</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row1\" class=\"row_heading level0 row1\" >lr</th>\n",
       "                        <td id=\"T_92272_row1_col0\" class=\"data row1 col0\" >Logistic Regression</td>\n",
       "                        <td id=\"T_92272_row1_col1\" class=\"data row1 col1\" >0.7925</td>\n",
       "                        <td id=\"T_92272_row1_col2\" class=\"data row1 col2\" >0.8334</td>\n",
       "                        <td id=\"T_92272_row1_col3\" class=\"data row1 col3\" >0.5185</td>\n",
       "                        <td id=\"T_92272_row1_col4\" class=\"data row1 col4\" >0.6280</td>\n",
       "                        <td id=\"T_92272_row1_col5\" class=\"data row1 col5\" >0.5672</td>\n",
       "                        <td id=\"T_92272_row1_col6\" class=\"data row1 col6\" >0.4327</td>\n",
       "                        <td id=\"T_92272_row1_col7\" class=\"data row1 col7\" >0.4365</td>\n",
       "                        <td id=\"T_92272_row1_col8\" class=\"data row1 col8\" >0.6010</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row2\" class=\"row_heading level0 row2\" >ada</th>\n",
       "                        <td id=\"T_92272_row2_col0\" class=\"data row2 col0\" >Ada Boost Classifier</td>\n",
       "                        <td id=\"T_92272_row2_col1\" class=\"data row2 col1\" >0.7917</td>\n",
       "                        <td id=\"T_92272_row2_col2\" class=\"data row2 col2\" >0.8322</td>\n",
       "                        <td id=\"T_92272_row2_col3\" class=\"data row2 col3\" >0.5000</td>\n",
       "                        <td id=\"T_92272_row2_col4\" class=\"data row2 col4\" >0.6341</td>\n",
       "                        <td id=\"T_92272_row2_col5\" class=\"data row2 col5\" >0.5585</td>\n",
       "                        <td id=\"T_92272_row2_col6\" class=\"data row2 col6\" >0.4247</td>\n",
       "                        <td id=\"T_92272_row2_col7\" class=\"data row2 col7\" >0.4302</td>\n",
       "                        <td id=\"T_92272_row2_col8\" class=\"data row2 col8\" >0.0770</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row3\" class=\"row_heading level0 row3\" >catboost</th>\n",
       "                        <td id=\"T_92272_row3_col0\" class=\"data row3 col0\" >CatBoost Classifier</td>\n",
       "                        <td id=\"T_92272_row3_col1\" class=\"data row3 col1\" >0.7897</td>\n",
       "                        <td id=\"T_92272_row3_col2\" class=\"data row3 col2\" >0.8321</td>\n",
       "                        <td id=\"T_92272_row3_col3\" class=\"data row3 col3\" >0.4769</td>\n",
       "                        <td id=\"T_92272_row3_col4\" class=\"data row3 col4\" >0.6339</td>\n",
       "                        <td id=\"T_92272_row3_col5\" class=\"data row3 col5\" >0.5434</td>\n",
       "                        <td id=\"T_92272_row3_col6\" class=\"data row3 col6\" >0.4107</td>\n",
       "                        <td id=\"T_92272_row3_col7\" class=\"data row3 col7\" >0.4180</td>\n",
       "                        <td id=\"T_92272_row3_col8\" class=\"data row3 col8\" >1.4740</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row4\" class=\"row_heading level0 row4\" >lda</th>\n",
       "                        <td id=\"T_92272_row4_col0\" class=\"data row4 col0\" >Linear Discriminant Analysis</td>\n",
       "                        <td id=\"T_92272_row4_col1\" class=\"data row4 col1\" >0.7907</td>\n",
       "                        <td id=\"T_92272_row4_col2\" class=\"data row4 col2\" >0.8235</td>\n",
       "                        <td id=\"T_92272_row4_col3\" class=\"data row4 col3\" >0.5254</td>\n",
       "                        <td id=\"T_92272_row4_col4\" class=\"data row4 col4\" >0.6205</td>\n",
       "                        <td id=\"T_92272_row4_col5\" class=\"data row4 col5\" >0.5683</td>\n",
       "                        <td id=\"T_92272_row4_col6\" class=\"data row4 col6\" >0.4317</td>\n",
       "                        <td id=\"T_92272_row4_col7\" class=\"data row4 col7\" >0.4346</td>\n",
       "                        <td id=\"T_92272_row4_col8\" class=\"data row4 col8\" >0.0110</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row5\" class=\"row_heading level0 row5\" >lightgbm</th>\n",
       "                        <td id=\"T_92272_row5_col0\" class=\"data row5 col0\" >Light Gradient Boosting Machine</td>\n",
       "                        <td id=\"T_92272_row5_col1\" class=\"data row5 col1\" >0.7848</td>\n",
       "                        <td id=\"T_92272_row5_col2\" class=\"data row5 col2\" >0.8234</td>\n",
       "                        <td id=\"T_92272_row5_col3\" class=\"data row5 col3\" >0.4838</td>\n",
       "                        <td id=\"T_92272_row5_col4\" class=\"data row5 col4\" >0.6178</td>\n",
       "                        <td id=\"T_92272_row5_col5\" class=\"data row5 col5\" >0.5414</td>\n",
       "                        <td id=\"T_92272_row5_col6\" class=\"data row5 col6\" >0.4039</td>\n",
       "                        <td id=\"T_92272_row5_col7\" class=\"data row5 col7\" >0.4097</td>\n",
       "                        <td id=\"T_92272_row5_col8\" class=\"data row5 col8\" >0.3030</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row6\" class=\"row_heading level0 row6\" >nb</th>\n",
       "                        <td id=\"T_92272_row6_col0\" class=\"data row6 col0\" >Naive Bayes</td>\n",
       "                        <td id=\"T_92272_row6_col1\" class=\"data row6 col1\" >0.7114</td>\n",
       "                        <td id=\"T_92272_row6_col2\" class=\"data row6 col2\" >0.8153</td>\n",
       "                        <td id=\"T_92272_row6_col3\" class=\"data row6 col3\" >0.7992</td>\n",
       "                        <td id=\"T_92272_row6_col4\" class=\"data row6 col4\" >0.4724</td>\n",
       "                        <td id=\"T_92272_row6_col5\" class=\"data row6 col5\" >0.5936</td>\n",
       "                        <td id=\"T_92272_row6_col6\" class=\"data row6 col6\" >0.3920</td>\n",
       "                        <td id=\"T_92272_row6_col7\" class=\"data row6 col7\" >0.4249</td>\n",
       "                        <td id=\"T_92272_row6_col8\" class=\"data row6 col8\" >0.0100</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row7\" class=\"row_heading level0 row7\" >xgboost</th>\n",
       "                        <td id=\"T_92272_row7_col0\" class=\"data row7 col0\" >Extreme Gradient Boosting</td>\n",
       "                        <td id=\"T_92272_row7_col1\" class=\"data row7 col1\" >0.7696</td>\n",
       "                        <td id=\"T_92272_row7_col2\" class=\"data row7 col2\" >0.8096</td>\n",
       "                        <td id=\"T_92272_row7_col3\" class=\"data row7 col3\" >0.4823</td>\n",
       "                        <td id=\"T_92272_row7_col4\" class=\"data row7 col4\" >0.5745</td>\n",
       "                        <td id=\"T_92272_row7_col5\" class=\"data row7 col5\" >0.5236</td>\n",
       "                        <td id=\"T_92272_row7_col6\" class=\"data row7 col6\" >0.3735</td>\n",
       "                        <td id=\"T_92272_row7_col7\" class=\"data row7 col7\" >0.3763</td>\n",
       "                        <td id=\"T_92272_row7_col8\" class=\"data row7 col8\" >0.3990</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row8\" class=\"row_heading level0 row8\" >rf</th>\n",
       "                        <td id=\"T_92272_row8_col0\" class=\"data row8 col0\" >Random Forest Classifier</td>\n",
       "                        <td id=\"T_92272_row8_col1\" class=\"data row8 col1\" >0.7730</td>\n",
       "                        <td id=\"T_92272_row8_col2\" class=\"data row8 col2\" >0.8037</td>\n",
       "                        <td id=\"T_92272_row8_col3\" class=\"data row8 col3\" >0.4692</td>\n",
       "                        <td id=\"T_92272_row8_col4\" class=\"data row8 col4\" >0.5870</td>\n",
       "                        <td id=\"T_92272_row8_col5\" class=\"data row8 col5\" >0.5208</td>\n",
       "                        <td id=\"T_92272_row8_col6\" class=\"data row8 col6\" >0.3748</td>\n",
       "                        <td id=\"T_92272_row8_col7\" class=\"data row8 col7\" >0.3791</td>\n",
       "                        <td id=\"T_92272_row8_col8\" class=\"data row8 col8\" >0.1720</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row9\" class=\"row_heading level0 row9\" >et</th>\n",
       "                        <td id=\"T_92272_row9_col0\" class=\"data row9 col0\" >Extra Trees Classifier</td>\n",
       "                        <td id=\"T_92272_row9_col1\" class=\"data row9 col1\" >0.7604</td>\n",
       "                        <td id=\"T_92272_row9_col2\" class=\"data row9 col2\" >0.7805</td>\n",
       "                        <td id=\"T_92272_row9_col3\" class=\"data row9 col3\" >0.4823</td>\n",
       "                        <td id=\"T_92272_row9_col4\" class=\"data row9 col4\" >0.5526</td>\n",
       "                        <td id=\"T_92272_row9_col5\" class=\"data row9 col5\" >0.5146</td>\n",
       "                        <td id=\"T_92272_row9_col6\" class=\"data row9 col6\" >0.3567</td>\n",
       "                        <td id=\"T_92272_row9_col7\" class=\"data row9 col7\" >0.3584</td>\n",
       "                        <td id=\"T_92272_row9_col8\" class=\"data row9 col8\" >0.1490</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row10\" class=\"row_heading level0 row10\" >knn</th>\n",
       "                        <td id=\"T_92272_row10_col0\" class=\"data row10 col0\" >K Neighbors Classifier</td>\n",
       "                        <td id=\"T_92272_row10_col1\" class=\"data row10 col1\" >0.7594</td>\n",
       "                        <td id=\"T_92272_row10_col2\" class=\"data row10 col2\" >0.7400</td>\n",
       "                        <td id=\"T_92272_row10_col3\" class=\"data row10 col3\" >0.4262</td>\n",
       "                        <td id=\"T_92272_row10_col4\" class=\"data row10 col4\" >0.5568</td>\n",
       "                        <td id=\"T_92272_row10_col5\" class=\"data row10 col5\" >0.4816</td>\n",
       "                        <td id=\"T_92272_row10_col6\" class=\"data row10 col6\" >0.3291</td>\n",
       "                        <td id=\"T_92272_row10_col7\" class=\"data row10 col7\" >0.3345</td>\n",
       "                        <td id=\"T_92272_row10_col8\" class=\"data row10 col8\" >0.0250</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row11\" class=\"row_heading level0 row11\" >dt</th>\n",
       "                        <td id=\"T_92272_row11_col0\" class=\"data row11 col0\" >Decision Tree Classifier</td>\n",
       "                        <td id=\"T_92272_row11_col1\" class=\"data row11 col1\" >0.7181</td>\n",
       "                        <td id=\"T_92272_row11_col2\" class=\"data row11 col2\" >0.6476</td>\n",
       "                        <td id=\"T_92272_row11_col3\" class=\"data row11 col3\" >0.4838</td>\n",
       "                        <td id=\"T_92272_row11_col4\" class=\"data row11 col4\" >0.4674</td>\n",
       "                        <td id=\"T_92272_row11_col5\" class=\"data row11 col5\" >0.4752</td>\n",
       "                        <td id=\"T_92272_row11_col6\" class=\"data row11 col6\" >0.2826</td>\n",
       "                        <td id=\"T_92272_row11_col7\" class=\"data row11 col7\" >0.2829</td>\n",
       "                        <td id=\"T_92272_row11_col8\" class=\"data row11 col8\" >0.0140</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row12\" class=\"row_heading level0 row12\" >qda</th>\n",
       "                        <td id=\"T_92272_row12_col0\" class=\"data row12 col0\" >Quadratic Discriminant Analysis</td>\n",
       "                        <td id=\"T_92272_row12_col1\" class=\"data row12 col1\" >0.6024</td>\n",
       "                        <td id=\"T_92272_row12_col2\" class=\"data row12 col2\" >0.6271</td>\n",
       "                        <td id=\"T_92272_row12_col3\" class=\"data row12 col3\" >0.6792</td>\n",
       "                        <td id=\"T_92272_row12_col4\" class=\"data row12 col4\" >0.3672</td>\n",
       "                        <td id=\"T_92272_row12_col5\" class=\"data row12 col5\" >0.4518</td>\n",
       "                        <td id=\"T_92272_row12_col6\" class=\"data row12 col6\" >0.1947</td>\n",
       "                        <td id=\"T_92272_row12_col7\" class=\"data row12 col7\" >0.2360</td>\n",
       "                        <td id=\"T_92272_row12_col8\" class=\"data row12 col8\" >0.0100</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row13\" class=\"row_heading level0 row13\" >svm</th>\n",
       "                        <td id=\"T_92272_row13_col0\" class=\"data row13 col0\" >SVM - Linear Kernel</td>\n",
       "                        <td id=\"T_92272_row13_col1\" class=\"data row13 col1\" >0.7655</td>\n",
       "                        <td id=\"T_92272_row13_col2\" class=\"data row13 col2\" >0.0000</td>\n",
       "                        <td id=\"T_92272_row13_col3\" class=\"data row13 col3\" >0.3815</td>\n",
       "                        <td id=\"T_92272_row13_col4\" class=\"data row13 col4\" >0.6028</td>\n",
       "                        <td id=\"T_92272_row13_col5\" class=\"data row13 col5\" >0.4448</td>\n",
       "                        <td id=\"T_92272_row13_col6\" class=\"data row13 col6\" >0.3126</td>\n",
       "                        <td id=\"T_92272_row13_col7\" class=\"data row13 col7\" >0.3338</td>\n",
       "                        <td id=\"T_92272_row13_col8\" class=\"data row13 col8\" >0.0140</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_92272_level0_row14\" class=\"row_heading level0 row14\" >ridge</th>\n",
       "                        <td id=\"T_92272_row14_col0\" class=\"data row14 col0\" >Ridge Classifier</td>\n",
       "                        <td id=\"T_92272_row14_col1\" class=\"data row14 col1\" >0.7911</td>\n",
       "                        <td id=\"T_92272_row14_col2\" class=\"data row14 col2\" >0.0000</td>\n",
       "                        <td id=\"T_92272_row14_col3\" class=\"data row14 col3\" >0.4608</td>\n",
       "                        <td id=\"T_92272_row14_col4\" class=\"data row14 col4\" >0.6435</td>\n",
       "                        <td id=\"T_92272_row14_col5\" class=\"data row14 col5\" >0.5362</td>\n",
       "                        <td id=\"T_92272_row14_col6\" class=\"data row14 col6\" >0.4066</td>\n",
       "                        <td id=\"T_92272_row14_col7\" class=\"data row14 col7\" >0.4163</td>\n",
       "                        <td id=\"T_92272_row14_col8\" class=\"data row14 col8\" >0.0090</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20e8bc4e6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#--Now to choose the metric\n",
    "#--------------------------\n",
    "\n",
    "from pycaret.classification import setup, compare_models, predict_model, save_model, load_model, tune_model, create_model\n",
    "\n",
    "automl = setup(df, target='Churn')  \n",
    "#------------------------------------\n",
    "automl\n",
    "\n",
    "best_model = compare_models(sort='AUC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f221fd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=6737, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--From this above, it appears that 'gbc' is the best choice for the data -- with the addition of \n",
    "#--charge_per_tenure column.  But I've run this a gazillion times, and I'm sticking with GBC.  \n",
    "#--\n",
    "#--79.19% accuracy\n",
    "#--\n",
    "#-- NOTE:  I'm getting a different listing each time I run this.  I speculate that there is a \"randomize\" component under\n",
    "#--        the hood on this, probably taking a seed from the time of day or something, which is slicing/randomizing the data\n",
    "#--        different ways.\n",
    "#--\n",
    "#--        I would also be willing to bet that if our training data were significantly larger, this \"randomizing\" thing would\n",
    "#--        stop being an issue.\n",
    "#----------------------------- \n",
    "best_model   #--Note,...it was GBC before.  It seems to flip between GBC, CatBoost and LR.  I'm sticking with GBC this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4e4c5",
   "metadata": {},
   "source": [
    "### 1b.  Choose a metric you think is best to use for finding the best model; by default, it is accuracy but it could be AUC, precision, recall, etc. \n",
    "\n",
    "#### The week 3 FTE has some information on these different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "432fc360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- I choose GradientBoostingClassifier\n",
    "#--\n",
    "#-- I am a believer that overall accuracy is important.  If someone gets a false-positive, they\n",
    "#-- could undergo additional testing. Same for false-negative.  But GBC seems to land the bests for AUC and Accuracy. \n",
    "#-------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc25075",
   "metadata": {},
   "source": [
    "### 2.  Save the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "689c8593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Succesfully Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(memory=None,\n",
       "          steps=[('dtypes',\n",
       "                  DataTypes_Auto_infer(categorical_features=[],\n",
       "                                       display_types=True, features_todrop=[],\n",
       "                                       id_columns=[],\n",
       "                                       ml_usecase='classification',\n",
       "                                       numerical_features=[], target='Churn',\n",
       "                                       time_features=[])),\n",
       "                 ('imputer',\n",
       "                  Simple_Imputer(categorical_strategy='not_available',\n",
       "                                 fill_value_categorical=None,\n",
       "                                 fill_value_numerical=None,\n",
       "                                 numeric_strate...\n",
       "                                             learning_rate=0.1, loss='deviance',\n",
       "                                             max_depth=3, max_features=None,\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=100,\n",
       "                                             n_iter_no_change=None,\n",
       "                                             presort='deprecated',\n",
       "                                             random_state=6737, subsample=1.0,\n",
       "                                             tol=0.0001, validation_fraction=0.1,\n",
       "                                             verbose=0, warm_start=False)]],\n",
       "          verbose=False),\n",
       " 'gbc.pkl')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(best_model, 'gbc')\n",
    "\n",
    "#--I confirmed, my model is in here --> \"C:\\Users\\Rob4H\\MSDS600_X70 -- Week 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e147554d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('dtypes',\n",
       "                 DataTypes_Auto_infer(categorical_features=[],\n",
       "                                      display_types=True, features_todrop=[],\n",
       "                                      id_columns=[],\n",
       "                                      ml_usecase='classification',\n",
       "                                      numerical_features=[], target='Churn',\n",
       "                                      time_features=[])),\n",
       "                ('imputer',\n",
       "                 Simple_Imputer(categorical_strategy='not_available',\n",
       "                                fill_value_categorical=None,\n",
       "                                fill_value_numerical=None,\n",
       "                                numeric_strate...\n",
       "                                            learning_rate=0.1, loss='deviance',\n",
       "                                            max_depth=3, max_features=None,\n",
       "                                            max_leaf_nodes=None,\n",
       "                                            min_impurity_decrease=0.0,\n",
       "                                            min_impurity_split=None,\n",
       "                                            min_samples_leaf=1,\n",
       "                                            min_samples_split=2,\n",
       "                                            min_weight_fraction_leaf=0.0,\n",
       "                                            n_estimators=100,\n",
       "                                            n_iter_no_change=None,\n",
       "                                            presort='deprecated',\n",
       "                                            random_state=6737, subsample=1.0,\n",
       "                                            tol=0.0001, validation_fraction=0.1,\n",
       "                                            verbose=0, warm_start=False)]],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--This is for my own debugging.  I don't know how to put a model into GITHUB and then retrieve it.\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "loaded_model = load_model('C:/Users/Rob4H/MSDS600_X70 -- Week 5/gbc')\n",
    "loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "de6eeeae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>charge_per_tenure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9305-CKSKC</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>97.40</td>\n",
       "      <td>811.70</td>\n",
       "      <td>36.895455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452-KNGVK</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>77.30</td>\n",
       "      <td>1701.95</td>\n",
       "      <td>212.743750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6723-OKKJM</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.25</td>\n",
       "      <td>250.90</td>\n",
       "      <td>8.960714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7832-POPKP</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>101.70</td>\n",
       "      <td>3106.56</td>\n",
       "      <td>50.105806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6348-TACGU</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51.15</td>\n",
       "      <td>3440.97</td>\n",
       "      <td>344.097000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tenure  PhoneService  Contract  PaymentMethod  MonthlyCharges  \\\n",
       "customerID                                                                  \n",
       "9305-CKSKC      22             1         0              2           97.40   \n",
       "1452-KNGVK       8             0         1              1           77.30   \n",
       "6723-OKKJM      28             1         0              0           28.25   \n",
       "7832-POPKP      62             1         0              2          101.70   \n",
       "6348-TACGU      10             0         0              1           51.15   \n",
       "\n",
       "            TotalCharges  charge_per_tenure  \n",
       "customerID                                   \n",
       "9305-CKSKC        811.70          36.895455  \n",
       "1452-KNGVK       1701.95         212.743750  \n",
       "6723-OKKJM        250.90           8.960714  \n",
       "7832-POPKP       3106.56          50.105806  \n",
       "6348-TACGU       3440.97         344.097000  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b5ca17e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#--It should be [1,0,0,1,0]\n",
    "#--I'm getting  [0,0,0,0,0]  --don't know if I'm close because what I've gotten is erratic.\n",
    "#--\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "my_list_of_values = []\n",
    "for i in range(5):\n",
    "   output = predict_model(loaded_model, new_data.iloc[i:i+1])\n",
    "   #print(output)\n",
    "   my_list_of_values.append(output['Label'].values[0])\n",
    "#-----------------------------------------------------\n",
    "print(my_list_of_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62223d",
   "metadata": {},
   "source": [
    "# Robin, I'm not please with the outcomes I'm getting.  \n",
    "# My accuracy should be higher than its showing, but I can't \n",
    "# seem to get past it.\n",
    "#\n",
    "# Regardless, I'm also going to check the old fashioned way of training the \n",
    "# model.  Since I have GBC as the chosen one, that is what I'm sticking with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d2dab0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customerID\n",
       "7590-VHVEG    0\n",
       "5575-GNVDE    0\n",
       "3668-QPYBK    1\n",
       "7795-CFOCW    0\n",
       "9237-HQITU    1\n",
       "             ..\n",
       "6840-RESVB    0\n",
       "2234-XADUH    0\n",
       "4801-JZAZL    0\n",
       "8361-LTMKD    1\n",
       "3186-AJIEK    0\n",
       "Name: Churn, Length: 7043, dtype: int64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--I'm gunna run this the old fashioned way.  Load it all up from the beginning\n",
    "#----------\n",
    "DV = \"Churn\"\n",
    "\n",
    "X_train = df.drop(DV, axis=1) #--this removes the \"Dependent Variable\" (DV) from our domain\n",
    "y_train = df[DV]   #--This has our DV in the range\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "818edcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "gbc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f4c6eebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "my_list_of_values = []\n",
    "for i in range(5):\n",
    "   output = gbc.predict(new_data.iloc[i:i+1])\n",
    "   my_list_of_values.append(output[0])\n",
    "\n",
    "print(my_list_of_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "7f3362ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy is 60.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [1, 0, 0, 1, 0]\n",
    "y_true = [0, 0, 0, 0, 0]\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Our accuracy is {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450407e4",
   "metadata": {},
   "source": [
    "### 3. create a Python script/file/module with a function that takes a pandas dataframe as an input and returns the probability of churn for each row in the dataframe\n",
    "* Your Python file/function should print out the predictions for new data (new_churn_data.csv)\n",
    "* The true values for the new data are [1, 0, 0, 1, 0] if you're interested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--https://github.com/Rob4Hope/MSDS600_X70/blob/main/Week_5_Python_Code.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f02464",
   "metadata": {},
   "source": [
    "### 4. Test your Python module and function with the new data, new_churn_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Things work.  I'm not happy with the accuracy. SERIOUSLY think something is broken on my packages or versions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86362f",
   "metadata": {},
   "source": [
    "### 5. write a short summary of the process and results at the end of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Well...I learned a few things, and I hope to learn a few more when this assignment gets looked at from other perspectives.\n",
    "#--\n",
    "#-- 1.  Setting things up wasn't as hard as I supposed, once I started to gete a feel for it.\n",
    "#-- 2.  I ran into bugs in my PyCharm installation, and had to backout pandas to an earlier version.\n",
    "#-- 3.  I ran into all kinds of problems with pycaret inside PyCharm, and since the assignment didn't say I had to use the\n",
    "#--     pycaret tools (like predict_model) inside my Python file, I chose to load the thing the old fashioned way.\n",
    "#-- 4.  My numbers were consistently bad.  I had high 'score' values, but my 'label' didn't move.  So, I'm missing something\n",
    "#--     something somewhere.  \n",
    "#-- 5.  I understand the process of using pycaret to compare models and make a choice.  The ramdoness of the model selection\n",
    "#--     was also intersting. I actually got some model estimates as high as 82%....but generally they were in the 79% range.\n",
    "#--\n",
    "#-- I spend a lot of time on this assignment.  I certainly understand the concept of what is being done.  But somewhere I \n",
    "#-- am worried I missed a parameter or something, which caused the low accuracy. I'm also VERY MUCH AWARE that the versions I\n",
    "#-- have installed MAY BE RESPONSIBLE for the troubles.  <<sigh>>\n",
    "#--\n",
    "#-- Anyway,...I'm turning it in.  Feedback is welcome..... :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd73c1",
   "metadata": {},
   "source": [
    "### 6. upload this Jupyter Notebook and Python file to a Github repository, and turn in a link to the repository in the week 5 assignment dropbox"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
